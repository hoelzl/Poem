<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Poem</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>Poem</h1>
        <h2>A language for designing awareness mechanisms</h2>
        <a href="https://github.com/hoelzl/Poem" class="button"><small>View project on</small> GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <p>Poem is a language for designing awareness mechanisms for adaptive,
autonomous systems.</p>

<p>Wait, what does that mean, exactly?</p>

<h2 id="autonomy-awareness-and-self-expression">Autonomy, Awareness and Self-Expression</h2>

<p>Many software-intensive systems deployed today are large and complex,
and they are often connected to the Internet which is even larger and
more complex. It is simply no longer possible to have operators
continuously monitor the components of a system, tune it to the specific
circumstances it encounters or diagnose malfunctions—these are things
systems should do on their own. Put another way, systems should be
adaptive and able to operate autonomously.</p>

<p>Let us look at an example: a swarm of rescue robots. Imagine that
after a natural disaster or an accident in an industrial plant, we can
deploy robots that aid the human rescue parties by, e.g., exploring
and mapping the terrain, securing dangerous structures, locating
victims and transporting them to safety, or by sealing off radioactive
or toxic substances. Clearly this has many benefits even if the robots
are not autonomous but have to be remotely controlled by humans. But
the environment in disaster areas is often not conductive to remote
control, as wires are likely to become physically damaged and wireless
reception is often poor. In addition, human experts are already under
enormous pressure, so it would be advantageous if the robot swarm
could operate autonomously. This clearly requires the robots to adapt
to a wide range of different, dynamically changing situations.</p>

<p>The big question is, of course, how can we build these kinds of
systems?  And in particular, how can we build them to do the right
thing even when they encounter circumstances that the designers have
not foreseen, or, as is the case for the hypothetical rescue robots,
when the designers don’t even know very much about the environment in
which the system will be operating. After all, the rescue robots are
not much help, if their actions endanger the victims or rescue
personnel.</p>

<p>There are many ways to answer this question, from mostly reactive
systems to sophisticated reasoners operating on vast knowledge bases.
And most of them are probably correct, at least for certain scenarios
and use cases.  There are, however, certain features that <em>all</em>
systems that can reasonably be called autonomous and adaptive share:
they need some way to store information, some way to receive
information from their environment and, unless we are dealing with
pure sensor systems, some way to influence their environment.  In
addition a system has to have some mechanism for answering queries
against the stored information, otherwise it might as well not bother
to store the information in the first place.  Simple query mechanisms
may do nothing but access the information stored in the system’s
memory, more sophisticated mechanisms may involve complex reasoning or
simulation steps.  Since we want to treat all systems in a unified
manner we assume that the system contains a <em>reasoning mechanism</em> of
some form.  The simple query mechanism would employ a reasoning
mechanism that performs no reasoning at all, but that’s OK. To make
the problem sound more grandiose and scientific I call these
components together the <em>awareness mechanism</em> of a system.</p>

<p>An awareness mechanism can be realized in a myriad of ways:
information can be stored in a centralized location, it can be
distributed between agents, or it may even be stored in the
environment.  As mentioned before, the reasoner may be trivial or
sophisticated.  The interface between the awareness mechanism and the
rest of the system may be formally defined, or it may be difficult to
say where the boundary of the awareness mechanism ends, as in the case
of the immune and nervous system of animals.  The book
<a href="http://thecomputerafterme.eu/">The Computer after Me</a> contains a much
more in-depth discussion of these topics.</p>

<p>So we now know <em>what</em> <em>Poem</em> is supposed to be good for: building
awareness mechanisms, i.e., the “brains” of adaptive, autonomous
systems.  This, of course, leads immediately to the question <em>how</em>
might <em>Poem</em> support this.  Why don’t we use any old programming
language to write our awareness mechanism?</p>

<p>The answer to this questions is twofold and the first half may be
either reassuring or off-putting, depending on whether you like
programming or not.  Firstly, when using <em>Poem</em> you still use any old
programming language to write your system.  Currently I have written
implementations of <em>Poem</em> in Lua, in Julia and in Lisp (that covers
all the mainstream languages, right?) but there is no reason why
<em>Poem</em> might not be implemented on top of C, C++, Java or any other
language (although the Lua implementation provides a very nice
solution for C/C++-based programs).  You just add a layer on top of
the language that structures your program as a tree of behaviors.
Behaviors are similar to functions with a particular simple interface:
each behavior gets the same types of arguments and returns the same
types of results.  The standardized interface serves the same purpose
that standardized containers play for shipping goods: while the
insides of your boxes may contain wildly different things the
infrastructure does not have to care about this at all.  Inside the
user-built “behavior boxes” you may perform any activity you like, in
any way you like; from the outside they all look the same.</p>

<p>For most systems that we build we have a definite idea what the system
should achieve (even if we don’t know how to implement a solution, or
even what a solution might look like).  Let’s assume that we can, at
least in theory, write down a (possibly time dependent) utility
function that formalizes this vague idea and imbue our system with
this utility function.  We would then like our system to use its
awareness mechanism in order to improve this utility function.  If it
actually does this we call the system <em>self expressive</em>.  In many
cases it seems to be self expression, rather than awareness itself,
that seems to pose the biggest challenges for system designers.</p>

<h2 id="here-no-there-no-everywhere">Here, no there, no everywhere…</h2>

<p>Of course the image of helpful robotic assistants that valiantly
rescue disaster victims without human oversight is not particularly
realistic.  Even in controlled conditions robots struggle to perform
much simpler tasks as soon as they require adaptation to unforeseen
circumstances, in spite of the tremendous progress that has been made
in many areas of artificial intelligence and machine learning.</p>

<p>There are many reasons for this, but I want to focus on one particular
aspect: in most situation where adaptation is required there is a huge
space of possible inferences we can draw and behaviors we can
try—almost all of them irrelevant to the problem.  If, for example,
you use a theorem prover to reason about desirable robot behaviors and
you have a sufficiently rich vocabulary in your logic, then you will
often observe the following phenomenon: You input a query how to
navigate from <em>A</em> to <em>B</em> to the reasoner, and it starts with reasoning
steps that seem natural enough.  But then it stumbles upon some
formula that connects locations with activities, and soon your
reasoner ponders questions such as “Will picking up a victim in
location <em>C</em> result in a higher reward than assisting robot <em>X</em>
transport a victim from <em>D</em> to <em>E</em>?”  While this question may be
interesting, it will not really help you solve your navigation problem
(and it includes another navigation problem of its own).  Instead of
staying focused on getting from here to there the reasoner goes
everywhere to find a solution.</p>

<p>This is, obviously, in no way a novel observation, and many ways to
tame the problem of “proof state explosion” have been proposed for
inference engines: set of support strategies that limit new inference
steps to predicates that occur in the query or in previous inference
steps; predicate ordering strategies, pruning branches in the proof
tree by rewriting, etc.</p>

<p>But one of the most interesting contributions to the problem of
guiding reasoning comes from the theory of
<a href="http://www.ncbi.nlm.nih.gov/pubmed/23663408">predictive processing</a>
that was proposed by neuroscientists and works in a very different
manner: it changes the problem from reasoning to prediction.  The
traditional model of “intilligent” systems supposes that sensors
deliver raw data to a reasoning system that processes this data and
sends the actions that should be performed to the actuators of a
system (this possibly happens in several layers).  Predictive
processing in contrast supposes that neuronal processing happens in a
multi-layer network, in which the higher layers generate predictions
of the sensor data and pass these predictions to the lower layers.
The lower layers, in turn, pass the error of these predictions up.
Predictive processing is closely connected to an area of machine
learning called <a href="http://deeplearning.net/">deep learning</a> that has
been applied very successfully to a number of hard machine learning
problems.  And in fact the Poem runtime (called <em>Iliad</em>) contains not
one but two third-party frameworks for deep learning (one written in
Lisp for easy integration with the reasoners, one in LuaJIT for easy
integration with the rest of the world).</p>

<p>So we’re all set, right?  Just put everything into a deep learner, and
obtain an intelligent agent.</p>

<p>As you surely know, this is not going to work for a number of
reasons, for example:</p>

<ul>
  <li>
    <p>it’s not clear how to even describe the behavior of a robot in such
a way that you can apply learning techniques to it;</p>
  </li>
  <li>
    <p>learning everything a robot needs to do takes a lot of time;</p>
  </li>
  <li>
    <p>relying on behaviors learned autonomously leaves developers very
little control over the system and is likely to result in many
unwanted behaviors;</p>
  </li>
  <li>
    <p>currently available techniques work rather poorly for learning
distributed behaviors.</p>
  </li>
</ul>

<h2 id="poem">Poem</h2>

<p>To build self-aware robots that exhibit useful behaviors we want to
combine fixed behavior with learning and reasoning in a flexible
manner: Behaviors for which the designers have good solutions can be
specified at design time; choices where the designers are not certain
which behavior is appropriate should be learned by experimentation at
run time; designers should be able to include guesses about
appropriate behaviors into learning or reasoning processes.  And we
want to be compatible with the main idea from predictive processing
which we summarize as “predict state or behavior at a high level; pass
this prediction down to more and more detailed behaviors, and pass
error values up.</p>

<p>The mechanism to achieve this in <em>Poem</em> are <em>partial programs</em>,
programs in which some parts are (more or less) unspecified.  The
unspecified parts are connected to learning or reasoning engines that
are responsible for computing the <em>completion</em> of the partial program,
i.e., a program in which all unspecified decisions have been
resolved.  To take good decisions it is often necessary to plan or
to simulate the effects of certain decisions.  Therefore <em>Poem</em> allows
the <em>virtualization</em> of the program state - the program state is
decoupled from the programs’s environment and effects are only applied
to the local state.  This allows the easy integration of planning and
simulations into programs.</p>

<p>Surely this must be very complicated?  Fortunately the answer is a
resounding “it depends”.  In <em>Poem</em> partial programs are structured as
extended behavior trees (XBTs) which are very easy to understand.  So
the basic structure of programs and the effects of virtualization and
repeated execution of program parts is easy to visualize.  However,
different learning, reasoning and planning engines introduce
complexities of their own that typically cannot be hidden from the
programmer: Using reinforcement techniques requires almost no work or
deep understanding on the part of the system designers (although
tuning them to achieve good performance does); using run-time theorem
proving requires quite a bit of background in logic and automated
theorem proving to avoid running into all kinds of complexity.</p>

<p>(Until October 2014 the answer would have been an unqualified
“unfortunately yes”: Previous versions of <em>Poem</em> used a
non-deterministic general-purpose language to specify partial
programs; this reulted in many subtle interactions between features
and surprising behaviors.)</p>

<h2 id="continuing">Continuing</h2>

<p>The easiest way to start writing <em>Poem</em> programs, is with XBTs in Lua.
The <a href="/xbt.html">Introduction to XBTs</a> contains a general
introduction to XBTs; the <a href="/quickstart.html">quickstart</a>
contains a “Getting Started” guide.  Details about using XBTs can be
found in the
<a href="/xbt-doc/modules/xbt.html">API documentation for XBTs</a>.</p>

<h2 id="acknowledgments">Acknowledgments</h2>

<p>The development of Poem was funded by the European project IP 257414
ASCENS (“Software Engineering for <strong>A</strong>utonomic
<strong>S</strong>ervice-<strong>C</strong>omponent <strong>Ens</strong>embles”).</p>

        </section>

	<aside id="sidebar">
	  <p>
	  <a href="index.html">Poem</a><br>
	  <a href="xbt.html">Extended Behavior Trees</a><br>
	  <a href="quickstart.html">Quickstart</a><br>
	  <a href="xbt-doc/modules/xbt.html">API Reference</a><br>	
	  </p>
          <a href="https://github.com/hoelzl/Poem/zipball/master" class="button">
            <small>Download</small>
            .zip file
          </a>
          <a href="https://github.com/hoelzl/Poem/tarball/master" class="button">
            <small>Download</small>
            .tar.gz file
          </a>

          <p class="repo-owner">The <a
          href="https://github.com/hoelzl/Poem">Poem GitHub Repository</a>
          is maintained
          by <a href="https://github.com/hoelzl">Matthias Hölzl</a>.</p>

          <p>This page was generated by <a href="http://jekyllrb.com">Jekyll</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>
